groups:
  - name: homelab_alerts
    interval: 1m
    rules:
      # LXC Container Down
      - alert: LXCContainerDown
        expr: pve_up{id=~"lxc/.*"} < 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "LXC Container is not running"
          description: "LXC container {{ $labels.name }} ({{ $labels.id }}) is down"

      # LXC High CPU Usage
      - alert: LXCHighCPUUsage
        expr: pve_cpu_usage_ratio{id=~"lxc/.*"} > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LXC CPU usage above 90% for 5 minutes"
          description: "LXC container {{ $labels.name }} CPU usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      # LXC High Memory Usage
      - alert: LXCHighMemoryUsage
        expr: pve_memory_usage_bytes{id=~"lxc/.*"} / pve_memory_size_bytes{id=~"lxc/.*"} > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LXC memory usage above 90% for 5 minutes"
          description: "LXC container {{ $labels.name }} memory usage is {{ $value | humanizePercentage }} (threshold: 90%)"

      # LXC Disk Space Low
      - alert: LXCDiskSpaceLow
        expr: (pve_disk_usage_bytes{id=~"lxc/.*"} / pve_disk_size_bytes{id=~"lxc/.*"}) > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "LXC disk usage above 85%"
          description: "LXC container {{ $labels.name }} disk usage is {{ $value | humanizePercentage }} (threshold: 85%)"

      # Proxmox Host High CPU
      - alert: ProxmoxHostHighCPU
        expr: pve_cpu_usage_ratio{type="node"} > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Proxmox host CPU usage above 85%"
          description: "Proxmox host {{ $labels.node }} CPU usage is {{ $value | humanizePercentage }} for 10 minutes"

      # Proxmox Host High Memory
      - alert: ProxmoxHostHighMemory
        expr: (pve_memory_usage_bytes{type="node"} / pve_memory_size_bytes{type="node"}) > 0.85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Proxmox host memory usage above 85%"
          description: "Proxmox host {{ $labels.node }} memory usage is {{ $value | humanizePercentage }} for 10 minutes"

      # Container Out of Memory Event
      - alert: ContainerOOMEvent
        expr: rate(container_oom_events_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m]) > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Container killed due to out-of-memory"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} experienced an OOM event"

      # Prometheus Target Down
      - alert: PrometheusTargetDown
        expr: up < 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Monitoring target is unreachable"
          description: "Prometheus target {{ $labels.instance }} ({{ $labels.job }}) is down"

      # Container High CPU Usage
      - alert: ContainerHighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m]) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container CPU usage above 90% for 10 minutes"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} CPU usage is {{ $value | printf \"%.1f\" }}%"

      # Container Restarts
      - alert: ContainerRestarting
        expr: rate(container_start_time_seconds{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container is restarting frequently"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has restarted {{ $value | printf \"%.2f\" }} times in 15 minutes"

      # Container High Memory Usage
      # DISABLED: Docker containers in LXC don't have memory limits set, causing false positives
      # Use LXCHighMemoryUsage instead for monitoring memory at LXC level
      # - alert: ContainerHighMemoryUsage
      #   expr: container_memory_working_set_bytes{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"} / container_spec_memory_limit_bytes{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"} > 0.9
      #   for: 5m
      #   labels:
      #     severity: warning
      #   annotations:
      #     summary: "Container memory usage above 90% of limit for 5 minutes"
      #     description: "Container {{ $labels.name }} on {{ $labels.instance }} memory usage is {{ $value | humanizePercentage }} of limit"

      # Container Scrape Errors
      - alert: ContainerScrapeErrors
        expr: container_scrape_error{name!=""} > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Container metrics collection failing"
          description: "Failed to scrape metrics from container {{ $labels.name }} on {{ $labels.instance }}"

      # High Network Errors
      - alert: HighNetworkErrors
        expr: rate(container_network_receive_errors_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m]) + rate(container_network_transmit_errors_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container experiencing high network error rate"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has high network error rate: {{ $value }} errors/s"

      # Node Exporter Down
      - alert: NodeExporterDown
        expr: up{job="node-exporter"} < 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Node Exporter is down"
          description: "Node Exporter on {{ $labels.instance }} is unreachable"

      # High Filesystem Usage (from node-exporter)
      - alert: HighFilesystemUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*",mountpoint="/datapool"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*",mountpoint="/datapool"})) > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Filesystem usage above 85%"
          description: "Filesystem {{ $labels.mountpoint }} on {{ $labels.instance }} is {{ $value | humanizePercentage }} full"

      # Prometheus Data Directory Low Space
      - alert: PrometheusStorageLow
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.*",mountpoint="/datapool"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.*",mountpoint="/datapool"})) > 0.90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus storage filesystem is critically low"
          description: "Data pool filesystem is {{ $value | humanizePercentage }} full. Prometheus data may be affected."

      # Too Many Alerts Firing
      - alert: TooManyAlertsFiring
        expr: count(ALERTS{alertstate="firing"}) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Too many alerts are firing"
          description: "{{ $value }} alerts are currently firing, indicating a potential systemic issue"
