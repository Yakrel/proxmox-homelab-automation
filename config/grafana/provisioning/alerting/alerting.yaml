apiVersion: 1

# Grafana Alerting Provisioning
# This file configures alert rule provisioning

groups:
  - orgId: 1
    name: alerting
    folder: Homelab Alerts
    interval: 1m
    rules:
      - uid: lxc_down
        title: LXC Container Down
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: prometheus
            model:
              expr: pve_up{id=~"lxc/.*"}
              refId: A
          - refId: B
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                    type: lt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 1
                      - 0
                    type: within_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 1m
        annotations:
          description: "LXC container {{ $labels.name }} ({{ $labels.id }}) is down"
          summary: LXC Container is not running
        labels:
          severity: critical

      - uid: lxc_high_cpu
        title: LXC High CPU Usage
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: pve_cpu_usage_ratio{id=~"lxc/.*"}
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: avg
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "LXC container {{ $labels.name }} CPU usage is {{ $values.B.Value | humanizePercentage }} (threshold: 90%)"
          summary: LXC CPU usage above 90% for 5 minutes
        labels:
          severity: warning

      - uid: lxc_high_memory
        title: LXC High Memory Usage
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: pve_memory_usage_bytes{id=~"lxc/.*"} / pve_memory_size_bytes{id=~"lxc/.*"}
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: avg
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "LXC container {{ $labels.name }} memory usage is {{ $values.B.Value | humanizePercentage }} (threshold: 90%)"
          summary: LXC memory usage above 90% for 5 minutes
        labels:
          severity: warning

      - uid: container_oom
        title: Container Out of Memory Event
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: rate(container_oom_events_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m])
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 1m
        annotations:
          description: "Container {{ $labels.name }} on {{ $labels.instance }} experienced an OOM event"
          summary: Container killed due to out-of-memory
        labels:
          severity: critical

      - uid: prometheus_target_down
        title: Prometheus Target Down
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 120
              to: 0
            datasourceUid: prometheus
            model:
              expr: up
              refId: A
          - refId: B
            relativeTimeRange:
              from: 120
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 1
                    type: lt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 120
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                      - 0
                    type: within_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 2m
        annotations:
          description: "Prometheus target {{ $labels.instance }} ({{ $labels.job }}) is down"
          summary: Monitoring target is unreachable
        labels:
          severity: critical

      - uid: container_high_memory
        title: Container High Memory Usage
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: container_memory_working_set_bytes{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"} / container_spec_memory_limit_bytes{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: avg
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0.9
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Container {{ $labels.name }} on {{ $labels.instance }} memory usage is {{ $values.B.Value | humanizePercentage }} of limit"
          summary: Container memory usage above 90% of limit for 5 minutes
        labels:
          severity: warning

      - uid: container_scrape_errors
        title: Container Scrape Errors
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: prometheus
            model:
              expr: container_scrape_error{name!=""}
              refId: A
          - refId: B
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 60
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 0
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 1m
        annotations:
          description: "Failed to scrape metrics from container {{ $labels.name }} on {{ $labels.instance }}"
          summary: Container metrics collection failing
        labels:
          severity: warning

      - uid: high_network_errors
        title: High Network Errors
        condition: C
        data:
          - refId: A
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: prometheus
            model:
              expr: rate(container_network_receive_errors_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m]) + rate(container_network_transmit_errors_total{name!="",name!~"POD|cadvisor|prometheus.*|grafana|loki|promtail.*|watchtower.*"}[5m])
              refId: A
          - refId: B
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 10
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                      - A
                  reducer:
                    params: []
                    type: avg
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: A
              reducer: last
              type: reduce
              refId: B
          - refId: C
            relativeTimeRange:
              from: 300
              to: 0
            datasourceUid: __expr__
            model:
              conditions:
                - evaluator:
                    params:
                      - 10
                      - 0
                    type: outside_range
                  operator:
                    type: and
                  query:
                    params:
                      - C
                  reducer:
                    params: []
                    type: last
                  type: query
              datasource:
                type: __expr__
                uid: __expr__
              expression: B
              reducer: last
              settings:
                mode: dropNN
              type: threshold
              refId: C
        noDataState: NoData
        execErrState: Error
        for: 5m
        annotations:
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has high network error rate: {{ $values.B.Value }} errors/s"
          summary: Container experiencing high network error rate
        labels:
          severity: warning

contactPoints:
  - orgId: 1
    name: grafana-default-email
    receivers:
      - uid: default-receiver
        type: email
        settings:
          addresses: "noreply@localhost"
        disableResolveMessage: true

policies:
  - orgId: 1
    receiver: grafana-default-email
    group_by:
      - grafana_folder
      - alertname
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
